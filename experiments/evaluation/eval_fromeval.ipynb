{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CARLMountainCarEnv\"\n",
    "\n",
    "database = {\n",
    "    \"CARLMountainCarEnv\": {\n",
    "        \"folders_eval\": [\n",
    "            \"/home/benjamin/Dokumente/code/tmp/tntcomp/CARL/multirun/2023-05-15/14-14-41\",\n",
    "        ],\n",
    "        \"context_feature\": \"['gravity']\",\n",
    "    },\n",
    "    \"CARLPendulumEnv\": {\n",
    "        \"folders_eval\": [\n",
    "            \"/home/benjamin/Dokumente/code/tmp/tntcomp/CARL/multirun/2023-04-05/14-31-22\",\n",
    "            \"/home/benjamin/Dokumente/code/tmp/tntcomp/CARL/multirun/2023-04-05/14-30-21\",\n",
    "        ],\n",
    "        \"context_feature\": \"['l']\",\n",
    "    },\n",
    "    \"CARLDmcWalkerEnv\": {\n",
    "        \"folders_eval\": [\n",
    "            \"/home/eimer/Dokumente/git/CARL/multirun/2023-03-01/10-21-58\",\n",
    "            \"/home/eimer/Dokumente/git/CARL/multirun/2023-04-20/10-43-59\",\n",
    "            \"/home/eimer/Dokumente/git/CARL/multirun/2023-04-20/10-45-51\",\n",
    "        ],\n",
    "        \"context_feature\": \"['viscosity']\",\n",
    "    },\n",
    "}\n",
    "\n",
    "replacements_bounds = {\n",
    "    \"[0.9, 1.1]\": 0.1,\n",
    "    \"[0.75, 1.25]\": 0.25,\n",
    "    \"[0.5, 1.5]\": 0.5,\n",
    "    \"[1, 2.2]\": 2.2,\n",
    "    \"[1, 2.5]\": 2.5,\n",
    "}\n",
    "key_interval = \"$\\Delta_{rel}$\"\n",
    "key_visibility = \"visibility\"\n",
    "key_performance = \"return\"\n",
    "key_cf = \"context_sampler.context_feature_names\"\n",
    "\n",
    "context_feature = database[env_name][\"context_feature\"]\n",
    "folders_eval = database[env_name][\"folders_eval\"]\n",
    "\n",
    "\n",
    "from rich import print as printr\n",
    "from rich.progress import track\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import ast\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from wandb.sdk.wandb_helper import parse_config\n",
    "from experiments.common.utils.json_utils import lazy_json_load\n",
    "from experiments.evaluation.run_evaluation import find_multirun_paths\n",
    "import sys\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from typing import Union, Dict, Optional\n",
    "from pathlib import Path\n",
    "from experiments.evaluation.utils import recover_traincfg_from_wandb\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "\n",
    "def load_wandb_table(fn: Union[str, Path]) -> pd.DataFrame:\n",
    "    data = lazy_json_load(fn)\n",
    "    data = pd.DataFrame(data=np.array(data[\"data\"]), columns=data[\"columns\"])\n",
    "    return data\n",
    "\n",
    "\n",
    "reload_rpc: bool = True\n",
    "\n",
    "fn_config = \".hydra/config.yaml\"\n",
    "fn_wbsummary = \"wandb/latest-run/files/wandb-summary.json\"\n",
    "fn_wbconfig = \"wandb/latest-run/files/config.yaml\"\n",
    "\n",
    "dfs = []\n",
    "lost_paths = []\n",
    "for folder_eval in folders_eval:\n",
    "    printr(f\"Loading from {folder_eval}\")\n",
    "    rpc_fn = Path(\"rpc\" + Path(folder_eval).name + \".csv\")\n",
    "    if not rpc_fn.is_file():\n",
    "        reload_rpc = True\n",
    "\n",
    "    paths = find_multirun_paths(result_dir=folder_eval)\n",
    "\n",
    "    if reload_rpc:\n",
    "        rpc_list = []\n",
    "        for i, p in tqdm(enumerate(paths), total=len(paths)):\n",
    "            p = Path(p)\n",
    "            fn_cfg = p / fn_config\n",
    "            fn_wbsum = p / fn_wbsummary\n",
    "            fn_wbcfg = p / fn_wbconfig\n",
    "            if not fn_wbcfg.is_file() or not fn_wbsum.is_file() or not fn_cfg.is_file():\n",
    "                continue\n",
    "            cfg = OmegaConf.load(fn_cfg)\n",
    "            traincfg = recover_traincfg_from_wandb(fn_wbcfg)\n",
    "            summary = lazy_json_load(fn_wbsum)\n",
    "\n",
    "            if \"average_return\" in summary:\n",
    "                average_return = summary[\"average_return\"]\n",
    "            else:\n",
    "                average_return = None\n",
    "\n",
    "            if average_return is None:\n",
    "                lost_paths.append(p)\n",
    "                continue\n",
    "\n",
    "            entry = {\n",
    "                \"average_return\": average_return,\n",
    "            }\n",
    "\n",
    "            path_to_table = (\n",
    "                fn_wbsum.parent / summary[\"return_per_context_table\"][\"path\"]\n",
    "            )\n",
    "            return_per_context = load_wandb_table(path_to_table)\n",
    "\n",
    "            # Sort by context id so we can add repetition info\n",
    "            return_per_context.sort_values(by=\"context_id\", inplace=True)\n",
    "            n_contexts = return_per_context[\"context_id\"].nunique()\n",
    "            n_reps = len(return_per_context) // n_contexts\n",
    "            assert n_contexts * n_reps == len(return_per_context)\n",
    "            repetitions = np.concatenate([np.arange(0, n_reps)] * n_contexts)\n",
    "            return_per_context[\"rep\"] = repetitions\n",
    "\n",
    "            contexts_path = fn_wbsum.parent / summary[\"evalpost/contexts\"][\"path\"]\n",
    "            contexts = load_wandb_table(contexts_path)\n",
    "\n",
    "            visibility = traincfg.wandb.group\n",
    "\n",
    "            context_ids = return_per_context[\"context_id\"].apply(int).to_list()\n",
    "            contexts_to_table = pd.DataFrame(\n",
    "                [contexts.iloc[cidx] for cidx in context_ids]\n",
    "            )\n",
    "            for col in contexts_to_table.columns:\n",
    "                return_per_context[col] = contexts_to_table[col].to_numpy()\n",
    "            n = len(return_per_context)\n",
    "            # return_per_context[\"average_return\"] = [average_return] * n\n",
    "            return_per_context[key_visibility] = visibility\n",
    "            return_per_context[key_interval] = replacements_bounds.get(\n",
    "                str(traincfg.context_sampler.uniform_bounds_rel), -1\n",
    "            )\n",
    "            return_per_context[\"context_sampler.context_feature_names\"] = str(\n",
    "                traincfg.context_sampler.context_feature_names\n",
    "            )\n",
    "            return_per_context[\"seed\"] = traincfg.seed\n",
    "            return_per_context[\"algorithm\"] = traincfg.algorithm\n",
    "            rpc_list.append(return_per_context)\n",
    "\n",
    "            # if i == 10:\n",
    "            #     break\n",
    "\n",
    "    if reload_rpc:\n",
    "        if len(rpc_list) == 0:\n",
    "            printr(\"Folder is empty\")\n",
    "            continue\n",
    "        df_rpc = pd.concat(rpc_list)\n",
    "        df_rpc.to_csv(rpc_fn)\n",
    "    else:\n",
    "        df_rpc = pd.read_csv(rpc_fn)\n",
    "    dfs.append(df_rpc)\n",
    "\n",
    "df_rpc = pd.concat(dfs).reset_index(drop=True)\n",
    "df_rpc.sort_values(by=key_visibility, inplace=True)\n",
    "\n",
    "printr(\"Lost paths\")\n",
    "printr(lost_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_rpc.columns)\n",
    "print(df_rpc[\"context_sampler.context_feature_names\"].unique())\n",
    "print(df_rpc[\"visibility\"].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpc = df_rpc[df_rpc[\"context_sampler.context_feature_names\"] == context_feature]\n",
    "df_rpc = df_rpc[\n",
    "    df_rpc[\"visibility\"].isin([\"concat (all)\", \"concat (non-static)\", \"hidden\"])\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "sns.set_palette(\"colorblind\")\n",
    "# sns.set_palette(\"tab10\")\n",
    "sns.set(font_scale=1.25)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "printr(df_rpc.columns)\n",
    "key = key_visibility\n",
    "data = df_rpc\n",
    "\n",
    "# All data points\n",
    "hue = key_visibility\n",
    "x = key_interval\n",
    "y = key_performance\n",
    "\n",
    "\n",
    "title = \"No Aggregation\"\n",
    "title = None\n",
    "\n",
    "hue_order = [\"hidden\", \"concat (all)\", \"concat (non-static)\"]\n",
    "pal = sns.color_palette(\"tab10\", n_colors=3)\n",
    "ids = [2, 0, 1]\n",
    "palette = {k: pal[v] for k, v in zip(hue_order, ids)}\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4), dpi=300)\n",
    "ax = fig.add_subplot(111)\n",
    "ax = sns.histplot(\n",
    "    data=data,\n",
    "    x=y,\n",
    "    hue=hue,\n",
    "    stat=\"frequency\",\n",
    "    multiple=\"layer\",\n",
    "    element=\"step\",\n",
    "    hue_order=hue_order,\n",
    "    palette=palette,\n",
    "    ax=ax,\n",
    "    lw=2,\n",
    ")\n",
    "ax.set_title(title)\n",
    "ax.set_xlabel(\"Return\")\n",
    "# ax.set_xticks([-1500, -1000, -500, 0])\n",
    "sns.move_legend(ax, \"upper left\")\n",
    "# ax.get_legend()(loc=\"upper left\")\n",
    "fig.set_tight_layout(True)\n",
    "fig_fn = Path(f\"data/figures/benchmarking_test/test_{env_name}.pdf\")\n",
    "fig_fn.parent.mkdir(exist_ok=True, parents=True)\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches=\"tight\")\n",
    "printr(fig_fn)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IQM\n",
    "## Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rliable import library as rly\n",
    "from rliable import metrics\n",
    "import importlib\n",
    "import rliable.plot_utils\n",
    "\n",
    "importlib.reload(rliable.plot_utils)\n",
    "\n",
    "fig_fn = \"tmp/figures/iqm.png\"\n",
    "\n",
    "data = df_rpc\n",
    "\n",
    "if \"algorithm\" in data:\n",
    "    del data[\"algorithm\"]\n",
    "\n",
    "reps = 10000\n",
    "xlabel = \"\"\n",
    "\n",
    "metric_names = [\"Median\", \"IQM\", \"Mean\"]  # , 'Optimality Gap']\n",
    "aggregate_func = lambda x: np.array(\n",
    "    [\n",
    "        metrics.aggregate_median(x),\n",
    "        metrics.aggregate_iqm(x),\n",
    "        metrics.aggregate_mean(x),\n",
    "        # metrics.aggregate_optimality_gap(x)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "keep_cfs = [\"[]\", \"[m]\", \"[g]\", \"[l]\", \"[dt]\"]\n",
    "filter_visibilities = [\"concat (all)\", \"cgate_lstm\"]\n",
    "\n",
    "\n",
    "# Create score matrix\n",
    "# Scores: (num_points x num_tasks), here:\n",
    "#         (num_seeds * num_reps x num_tasks)\n",
    "# Pendulum: num_tasks = n_contexts = 128\n",
    "\n",
    "\n",
    "# Create score matrix\n",
    "# Scores: (num_points x num_tasks), here:\n",
    "# here:\n",
    "#     num_points = n_contexts * n_seeds * n_reps\n",
    "#     n_tasks = n_intervals * n_cfs\n",
    "score_dict = {}\n",
    "tasks = [key_interval, \"context_sampler.context_feature_names\"]\n",
    "points = [\"seed\", \"rep\", \"context_id\"]\n",
    "for group_id, group_df in data.groupby(\"visibility\"):\n",
    "    # Determine size of score matrix\n",
    "    def count(x):\n",
    "        counts = group_df[x].nunique()\n",
    "        # print(x, counts)\n",
    "        return counts\n",
    "\n",
    "    n_points = np.prod([count(x) for x in points])\n",
    "    n_tasks = np.prod([count(x) for x in tasks])\n",
    "    scores = np.zeros((n_points, n_tasks))\n",
    "    print(scores.shape)\n",
    "    for i, ((seed, rep, context_id), gdf) in enumerate(\n",
    "        group_df.groupby([\"seed\", \"rep\", \"context_id\"])\n",
    "    ):\n",
    "\n",
    "        # gdf.sort_values(by=\"context_id\", inplace=True)\n",
    "        n = len(gdf)\n",
    "        if n > n_tasks:\n",
    "            print(\n",
    "                f\"Warning! We have more tasks than specified. {n} > {n_tasks}. Capping.\"\n",
    "            )\n",
    "            new_df = []\n",
    "            for sid, sdf in gdf.groupby(tasks):\n",
    "                # printr(sid, len(sdf), sdf)\n",
    "                new_df.append(sdf.iloc[np.random.choice(np.arange(0, len(sdf)))])\n",
    "            gdf = pd.DataFrame(new_df)  # .reset_index(drop=True)\n",
    "            del gdf[\"Unnamed: 0\"]\n",
    "        elif n < n_tasks:\n",
    "            print(\n",
    "                f\"Warning! We have less tasks than specified. {n} > {n_tasks}. Skip.\",\n",
    "                seed,\n",
    "                rep,\n",
    "                context_id,\n",
    "            )\n",
    "            continue\n",
    "            # printr(gdf)\n",
    "\n",
    "        # print(n, seed, rep,)\n",
    "        # print(gdf[key_interval].nunique(), gdf[\"context_sampler.context_feature_names\"].nunique())\n",
    "        gdf.sort_values(by=\"context_sampler.context_feature_names\", inplace=True)\n",
    "        # printr(gdf)\n",
    "        # index = (seed - 1) * n_seeds + rep  # seed is 1-based\n",
    "        R = gdf[\"return\"]\n",
    "        scores[i] = R\n",
    "    score_dict[group_id] = scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=0.8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# filter_visibilities = [\"concat (all)\", \"cgate_lstm\"]\n",
    "# for vis in filter_visibilities:\n",
    "#     if vis in score_dict:\n",
    "#         del score_dict[vis]\n",
    "\n",
    "printr(\"Calculate interval estimates\")\n",
    "aggregate_scores, aggregate_score_cis = rly.get_interval_estimates(\n",
    "    score_dict, aggregate_func, reps=reps\n",
    ")\n",
    "\n",
    "printr(\"Plot interval estimates\")\n",
    "fig, axes = rliable.plot_utils.plot_interval_estimates(\n",
    "    aggregate_scores,\n",
    "    aggregate_score_cis,\n",
    "    metric_names=metric_names,\n",
    "    algorithms=list(score_dict.keys()),\n",
    "    xlabel=xlabel,\n",
    "    asrows=True,\n",
    "    row_height=0.5,\n",
    "    interval_height=0.75,\n",
    ")\n",
    "fig.set_tight_layout(True)\n",
    "fig.savefig(fig_fn, bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "445f316a49a09d784fc20e9b3e621eb4aa26f31c3cfbe2b15408208fc87e667a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
