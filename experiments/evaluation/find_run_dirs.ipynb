{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1043/1043 [00:11<00:00, 87.76it/s] \n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "env_name = \"CARLPendulumEnv\"\n",
    "experiment = \"benchmarking\"\n",
    "eval_data_fn = \"tmp/evaldata.pickle\"\n",
    "\n",
    "\n",
    "# def load_runs_from_wandb(env_name: str, eval_data_fn: str | Path, reload: bool = False, experiment: str = \"benchmarking\") -> pd.DataFrame:\n",
    "eval_data_fn = Path(eval_data_fn)\n",
    "if not eval_data_fn.is_file():\n",
    "    reload = True\n",
    "\n",
    "if reload:\n",
    "    api = wandb.Api()\n",
    "\n",
    "    # Project is specified by <entity/project-name>\n",
    "    filters = {\n",
    "        \"config.env\": env_name,\n",
    "        \"config.experiment\": experiment,\n",
    "        \"config.wandb.job_type\": \"train\",\n",
    "        # \"config.context_sampler.n_samples\": 1000,\n",
    "        \"state\": \"finished\",\n",
    "    }\n",
    "    metrics = [\"eval/return\", \"train/global_step\"]\n",
    "    runs = api.runs(\"tnt/carl-tmlr\", filters=filters)\n",
    "\n",
    "    summary_list, config_list, name_list, metrics_list = [], [], [], []\n",
    "    for run in tqdm(runs, total=len(runs)):\n",
    "        # # Check metrics first. If not all available, do not append run\n",
    "        # rows = []\n",
    "        # for i, row in run.history(keys=metrics).iterrows():\n",
    "        #     if all([metric in row for metric in metrics]):\n",
    "        #         # df = df.append(row, ignore_index=True)\n",
    "        #         rows.append(row)\n",
    "        #     else:\n",
    "        #         continue\n",
    "        # df = pd.DataFrame(rows)\n",
    "        # metrics_list.append(df)\n",
    "        \n",
    "        # .summary contains the output keys/values for metrics like accuracy.\n",
    "        #  We call ._json_dict to omit large files\n",
    "        summary_list.append(run.summary._json_dict)\n",
    "\n",
    "        # .config contains the hyperparameters.\n",
    "        #  We remove special values that start with _.\n",
    "        config_list.append(\n",
    "            {k: v for k,v in run.config.items()\n",
    "            if not k.startswith('_')})\n",
    "\n",
    "        # .name is the human-readable name of the run.\n",
    "        name_list.append(run.name)\n",
    "\n",
    "\n",
    "    runs_df = pd.DataFrame({\n",
    "        \"summary\": summary_list,\n",
    "        \"config\": config_list,\n",
    "        \"name\": name_list,\n",
    "        # \"metrics\": metrics_list,\n",
    "        })\n",
    "\n",
    "    runs_df.to_pickle(eval_data_fn)\n",
    "else:\n",
    "    runs_df = pd.read_pickle(eval_data_fn)\n",
    "    # return runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python experiments/evaluation/run_evaluation.py --result_dir '/home/benjamin/Dokumente/code/tmp/tntcomp/CARL/exp_sweep/2022-09-12/08-35-57_benchmark_train' -m\n",
      "python experiments/evaluation/run_evaluation.py --result_dir '/home/benjamin/Dokumente/code/tmp/tntcomp/CARL/exp_sweep/2022-09-12/12-02-53_benchmark_train' -m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rich import print as printr\n",
    "\n",
    "rootdir = Path(\"/home/benjamin/Dokumente/code/tmp/tntcomp/CARL\")\n",
    "\n",
    "result_dirs = []\n",
    "for index, run in runs_df.iterrows():\n",
    "    # slurm_id = run[\"config\"][\"slurm_id\"]\n",
    "    outdir = run[\"config\"][\"output_dir\"]\n",
    "    outdir = \"exp_sweep\"\n",
    "    wandb_id = run[\"config\"][\"wandb\"][\"id\"]\n",
    "    dirs = wandb_id.split(\"_\", maxsplit=1)\n",
    "    result_dir = rootdir / outdir / dirs[0] / dirs[1][:-7]\n",
    "    result_dirs.append(result_dir)\n",
    "\n",
    "result_dirs = np.unique(result_dirs)\n",
    "\n",
    "command_template = \"python experiments/evaluation/run_evaluation.py --result_dir '{result_dir}' -m\"\n",
    "commands = [command_template.format(result_dir=p) for p in result_dirs]\n",
    "for command in commands:\n",
    "    print(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('carl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "445f316a49a09d784fc20e9b3e621eb4aa26f31c3cfbe2b15408208fc87e667a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
